,topic,subreddit,title,score,id,url,created,body
0,CVE,learnpython,How to create Virus on Android device without Rooting.,1,i3x3os,https://www.reddit.com/r/learnpython/comments/i3x3os/how_to_create_virus_on_android_device_without/,2020-08-05 12:30:39,How to create Virus on Android device without Rooting. [click here](URL #CVE #infosec #CyberSecurity #whatsappstokvel #100DaysOfCode #javascript #programming #Python #Pentesting #HackLearning
1,CVE,learnpython,Need help understanding what I'm doing wrong with sqlite3.,1,igkpzr,https://www.reddit.com/r/learnpython/comments/igkpzr/need_help_understanding_what_im_doing_wrong_with/,2020-08-26 06:46:51,"I'm writing an app to quickly gather information about specific CVEs. Most of what I've done is working as I would expect it to. The issue I'm having is with inserting data into a sqlite3 database. This is the first time I've tried using sqlite3 in python. Here's my database, and script that I'm testing with [URL `rm cve.db &gt;/dev/null ; sqlite3 cve.db &lt; cve.sql` [URL `$ ./cve.py` `update_cve called with value CVE-2020-0001 of type &lt;class 'str'&gt;` `update_cve called with value CVE-2020-0002 of type &lt;class 'str'&gt;` `update_cve called with value CVE-2020-0003 of type &lt;class 'str'&gt;` `update_cve called with value CVE-2020-0004 of type &lt;class 'str'&gt;` and it will continue to output the same over multiple runs. I believe it should be adding the CVEs to the database the first time, and outputing the information the second time. If I make the same call outside of the loop (uncomment line 24) CVE-2020-0003 gets added to the database, and shows up with the info from the database. `$ ./cve.py` `update_cve called with value CVE-2020-0003 of type &lt;class 'str'&gt;` `update_cve called with value CVE-2020-0001 of type &lt;class 'str'&gt;` `update_cve called with value CVE-2020-0002 of type &lt;class 'str'&gt;` `update_cve called with value CVE-2020-0003 of type &lt;class 'str'&gt;` `('CVE-2020-0003', None, None)` `update_cve called with value CVE-2020-0004 of type &lt;class 'str'&gt;` I have no idea why it's not being created when it gets called from within the for loop. I don't post a lot in here, so I apologize if I've not provided my information well, or correctly. Thanks for looking!"
2,CVE,learnpython,Using column entries for one dataframe to search for matches in a second dataframe using Pandas.,1,hge28t,https://www.reddit.com/r/learnpython/comments/hge28t/using_column_entries_for_one_dataframe_to_search/,2020-06-27 05:20:20,"To preface, I am new to pandas but I think it could be useful for what I am trying to do. I have two .csv files that I have read into dataframes. The goal is to associate software names with open CVE IDs (vulnerabilities in the software). The first file is called SoftwareItems.csv, and it contains a list of application names in a single column, excerpt here: ApplicationName | ---| Java SDK 6 | Internet Explorer 7 | Visual Studio SDK 2010 | ...and so on. The second file is called CVElist.csv, and it contains the full list of CVE IDs and descriptions of each, excerpt here: Name | Description ---|--- CVE-1999-0001| ip_input.c in BSD-derived TCP/IP implementations allows remote attackers to cause a denial of service (crash or hang) via crafted packets. CVE-1999-0002 | Buffer overflow in NFS mountd gives root access to remote attackers, mostly in Linux systems. ...and so on. My code currently looks like this: ---- #Import Modules import pandas as pd #Read CSV Data software_df = pd.read_csv('SoftwareItems.csv', engine ='python') cve_df = pd.read_csv('CVElist.csv'. engine='python') #Create New Dataframe to Write filtered_df = software_df.copy() #Write to New DF for i in range (len(software_df)): for x in range (len(cve_df)): if cve_df['Description', x].str.contains(software_df['ApplicationName', i]): filtered_df['CVEids', i] = cve_df['Name', x] #Display Output print (filtered_df) ---- The main output I get: KeyError: ('Description', 0) The logic of what I am trying to do is as follows: - Start with first software name (first row of software_df['ApplicationName']) - Iterate through full list of CVE descriptions in the cve_df['Description'] column for matching software names (str.contains) - Each time a match is found, take the associated CVE ID at that current index (cve_df['Name']) and add that CVE ID to the new dataframe in column filtered_df['CVEids'] - Increment to next software name in software_df['ApplicationName'] - Iterate through full list of CVE descriptions again for any matching software names to add associated CVE IDs to filtered_df['CVEids'] - Repeat for all software names The expected output I am trying to arrive at should look like this: ApplicationName CVEids Adobe Flash CVE-2012-5678, CVE-2012-5677, (etc...) Adobe Reader CVE-2009-4324, CVE-2009-3462, (etc...) I have been banging my head against the wall all day on this, so any assistance would be greatly appreciated. Mods - this took FOREVER to write up and I did read guidelines beforehand, so please let me know if I have done something wrong here before removing. Thank you all!"
3,CVE,learnpython,Cloning GitHub Repos for Python,1,glgugu,https://www.reddit.com/r/learnpython/comments/glgugu/cloning_github_repos_for_python/,2020-05-18 01:02:50,"I have zero experience with Github, and I can't seem to find good information that answers my question easily. I provided an example repo below that I am trrying to download the zip of: URL Can I unpackage this anywhere and it should work just fine? In my mind I should take the folder inside and place it directly into: ""C:\Users\User\AppData\Local\Programs\Python\Python38-32"" The code itself references nested objects so the folder structure should not be modified right? Just looking for the easiest way to not mess up for a noob such as myself and others learning. Thanks!"
4,CVE,learnpython,"Question about unpacking buffers struct.unpack(format,buffer)",1,fo29a2,https://www.reddit.com/r/learnpython/comments/fo29a2/question_about_unpacking_buffers/,2020-03-24 20:31:39,"So I am reading this script that has that exact line: nb, = struct.unpack(""&gt;I"", sock.recv(4)) And according to the documentation, the string ""&gt;I"" is apparently a formatting convention: [URL However, I tried to search but I cannot find why the author chose the ""&gt;I"", it doesn't really make sense to me. My question is: what does ""&gt;I"" mean in this context? For reference, the script is here: [URL Thanks a lot!"
5,CVE,learnpython,Need help with my web scraper,1,begrhr,https://www.reddit.com/r/learnpython/comments/begrhr/need_help_with_my_web_scraper/,2019-04-18 13:13:44,"Hey guys, I am creating a web scraper to get data from this website: [URL &amp;#x200B; I am trying to scrape the word 'Critical' but I'm not sure how to reference it. I tried looking at the header using inspect element, and it says it is a &lt;span&gt; header but I still cant seem to reference it. &amp;#x200B; Here is my code: #Creating my own webscraper from urllib.request import urlopen as uReq from bs4 import BeautifulSoup as soup import urllib.request myurl = 'URL myReq = (myurl) req = urllib.request.Request( myurl, data=None, headers={ 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36' } ) #opening my connection, grabbing the page uClient = uReq(myurl) page_html = uClient.read() uClient.close() #html parsing page_soup = soup(page_html, 'html.parser') print(page_soup.span) BTW the output gives me a title from the website, but not the word 'Critical'. &amp;#x200B; Any help would be appreciated!"
6,CVE,learnpython,Having a hard time following links with a webcrawler,1,bgshxu,https://www.reddit.com/r/learnpython/comments/bgshxu/having_a_hard_time_following_links_with_a/,2019-04-24 19:24:22,"Hey guys, I created a web-scraper that scrapes two data points from a patchnote (Impactscore and Exploitabilityscore) This worked fine, and I managed to add this data to a csv file however I am having difficulties making my code do this for all patch notes. In theory the web crawler would start on the main page, click on one link, scrape the data and go back to the main page and do the same thing for the second link. Link of all patch notes : [URL &amp;#x200B; Link of one patch note that I scraped : [URL &amp;#x200B; Working code to scrape the data from one patch note: from urllib.request import urlopen as uReq from bs4 import BeautifulSoup as soup import urllib.request import xlwt from xlwt import Workbook wb = Workbook() #Creating the workbook sheet1 = wb.add_sheet('Data Code') myurl = 'URL myReq = (myurl) req = urllib.request.Request( myurl, data=None, headers={ 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36' } ) #opening my connection, grabbing the page uClient = uReq(myurl) page_html = uClient.read() uClient.close() #html parsing page_soup = soup(page_html, 'html.parser') BaseScore = page_soup.findAll('span', {'data-testid': 'vuln-cvssv3-base-score-severity'}) ImpactScore = page_soup.findAll('span', {'data-testid': 'vuln-cvssv3-impact-score'}) for e in ImpactScore: q = e.get_text().strip() q = float(q) print(q) sheet1.write(1,0,q) ImpactSubscore = page_soup.findAll('span', {'data-testid': 'vuln-cvssv2-impact-subscore'}) for e in ImpactSubscore: r = e.get_text().strip() r = float(r) print(r) sheet1.write(1,2,r) sheet1.write(0,0,'ImpactScore') sheet1.write(0,2, 'ExploitabilityScore') wb.save('Data.xls') &amp;#x200B; If anyone has any suggestions or advice, that would be great! Thanks if advance :)"
7,CVE,learnpython,"How to make a webcrawler that parses out links with the name ""patch"" or ""fix?""",3,ba9whd,https://www.reddit.com/r/learnpython/comments/ba9whd/how_to_make_a_webcrawler_that_parses_out_links/,2019-04-07 08:44:29,"I don't know how to do this very well. I'm trying to complete an application task for a Debian GSoC project, and I'm having a damn hard time trying to figure this out. I just got the parsing of the text file complete, but not parsing the list of pages grabbed from the text file. In particular, any page that from sourceware.org's BugZilla. Here's test code that contains the parsing each page part I'm stuck on: #!/usr/bin/env python3 This program uses Python 3, don't use with 2. import requests from bs4 import BeautifulSoup import re import os PAGES_CAH = [""URL ""URL ""URL ""URL ] patches = [] def searchy(pages): for link in pages: global patches if ""github.com"" in link and ""commit"" in link: # detect that in each page that it's from GitHub if 'patch' not in link: # detect if it's a patch page or not link = link + '.patch' # add .patch to link if the patch link lacks it request = requests.get(link) # connect to page patches.append(request.text) # download patch to patches variable elif "".patch"" in link: # any other page with "".patach"" in the end is downloaded like GitHub patches by default request = requests.get(link) # connect to page patches.append(request) #downmload patch to patches variable else: request = requests.get(link) # connect to page soup = BeautifulSoup(request.text, ""lxml"") # turn the page into something parsable if ""sourceware.org/git"" in link: # if it's from sourceware.org's git: patch_link = soup.find_all('a', string=""patch"") # find all patch links patch_request = requests.get(patch_link[0]) # connect to patch link patches.append(patch_request.text) # download patch elif ""sourceware.org/bugzilla"" in link: # if it's from sourceware's bugzilla patch_link_possibilities = soup.find('a', id=""attachment_table"") # find all links from the attachment table local_patches_links = patch_link_possibilities.find_all(string=""patch"") # find all links with the ""patch"" name local_fixes_links = patch_link_possibilities.find_all(string=""fix"") # find all links with the ""fix"" name for lolpatch in local_patches_links: # for each local patch in the local patch links list patch_request = requests.get(lolpatch) # connect to page patches.append(patch_request.text) #download patch for fix in local_fixes_links: # for each fix in the local fix links list patch_request = requests.get(fix) # connect to page patches.append(patch_request.text) #download patch searchy(PAGES_CAH) print(patches) Output when running is the following: Traceback (most recent call last): File ""test.py"", line 40, in &lt;module&gt; searchy(PAGES_CAH) File ""test.py"", line 32, in searchy local_patches_links = patch_link_possibilities.find_all(string=""patch"") # find all links with the ""patch"" name AttributeError: 'NoneType' object has no attribute 'find_all' I'm having problems with the first link, a link about sourceware.org's BugZilla, in particular, and I have no clue how to find links with the names ""patch"" and ""fix"" so I can download patches with this program."
8,CVE,learnpython,pre-existing class that describes a security vulnerability object?,1,ain0x5,https://www.reddit.com/r/learnpython/comments/ain0x5/preexisting_class_that_describes_a_security/,2019-01-23 00:52:50,"I could make the class I need, but figure someone smarter than me has already created a python class for instantiating a security vulnerability object. Is there a library of example python objects like cars, employees, books and... security vulnerabilities? Or any suggestions on how to find a pre-existing class that defines a generic security vulnerability? Background: Need to process security vulnerability info (CVE &amp; NVD info) from multiple vendors. I started creating a class to define a vulnerability object with attributes like name, summary, description, recommended fix, etc. and some custom methods like converting different vendor risk ratings into a standardized risk rating based on network location. My google-fu is failing me on this one. Googling for variations of python, classes and vulnerabilities is returning all kinds of training offerings, vuln announcements and other misdirections."
9,CVE,learnpython,"P27, How to unittest downloading a gzip file with request and request.content?",1,a1j1bb,https://www.reddit.com/r/learnpython/comments/a1j1bb/p27_how_to_unittest_downloading_a_gzip_file_with/,2018-11-30 03:51:11,"I am currently writing a piece of code that downloads a json gz file into memory/io, unzips it, and turns it into a dict. Here is a sample code (had to find a random json.gzip that was safe for others to test on) response = requests.get('URL compressed_file = io.BytesIO(response.content) json_data = json.loads(gzip.GzipFile(fileobj=compressed_file).read()) This issue is, I have no idea how to ""mock"" the response.content to be a gzip file. The host that does the unittesting lacks access to the internet. So I need to load a .gz file from the host, and place it as .gz so it can be loaded into memory to be decompressed and converted. I looked at something like this, but I don't see how to apply it to be a .gz file. def mock_response(payload, status_code): mock_response = mock.MagicMock(spec=requests.Response) mock_response.text = payload mock_response.status_code = status_code So I am looking for some assistance on how to mock the response.content to be a .gz file. This is some legacy code I am trying to improve so I am also sort of stuck on Py2"
10,CVE,learnpython,"Looking for a way to scan for most recent vulnerabilities, and send emails based on what type of vulns they are",4,8pdf9o,https://www.reddit.com/r/learnpython/comments/8pdf9o/looking_for_a_way_to_scan_for_most_recent/,2018-06-08 05:53:08,"There is a website called the National Vulnerability Database (URL and on their frontpage there is a list of the most recent critical vulnerabilities. I would like to be able to scan these most recent vulns, and based on their attack vectors (their attack vectors can be found in the actual vulnerability report, ex: URL and it'll say ""Attack vector: Network:"") I'd like for my script to send an automatic email to a certain department of my company to alert them about this vulnerability. I don't even know where to start with a project like this, or how difficult it is... I'm looking for some help on where to start and where I could find resources to help me with this"
11,CVE,learnpython,"Parsing an RSS feed using feedparser, and sending emails based on what results I get",1,8qv5jj,https://www.reddit.com/r/learnpython/comments/8qv5jj/parsing_an_rss_feed_using_feedparser_and_sending/,2018-06-14 05:14:09,"I'd like to make a more refined RSS reader I guess. So far this is what my code looks like: import feedparser # NVD RSS feed variable d = feedparser.parse('URL print(""================================"") print(d['feed']['title']) print(""================================"") for entry in d.entries: print(entry) right now this code will return rows of what seems to be dictionaries (I'm not sure). And it seems that one of the keys for the dictionary called 'title' has the information I'm after. 'title': 'CVE-2018-8132 (windows_10, windows_server_2016)' In this key value, there are strings that contain the word 'windows'. Based off of what the value is, so if it's either windows, linux, mysql, internet_explorer or any other value, i'd like to send an email to the appropriate department along with the link of the vulnerability link (entry.link) so they can check and make sure we would be protected against such vulns. Any ideas on how I could proceed here? Thanks "
12,CVE,learnpython,new to python and starting a project. Parsing XML to CSV,1,83y3j3,https://www.reddit.com/r/learnpython/comments/83y3j3/new_to_python_and_starting_a_project_parsing_xml/,2018-03-13 07:12:43,"I went on github and saw a python script that I want to fix up. At my job I just built a vuln scanner but the reports are kind of meh. I want to get some more information out of the reports. Link: URL The XML files have all the information I want, so I figure this is a great time to start learning on how to learn how to script. What I want is the columns IP Addres FQDN Vulnerability CVE CVE Published Severity. Right now this github page I found only lists IP Address, Vuln, and Severity. I am playing around with the script and have only been able to create the columns, but can't seem to rip the rest from the XML. My issue is, the script seems to be making calls on pretty easy attributes, it'll call parent, and ask for the name and thats really about it. The CVE and Dates are all nested under a bunch of tags tag name = fqdn tag name = published date so on and so forth. I am a bit confused how I can get the script to differentiate between the tag name, I seem to understand just by reading the script how to get to point it to parent/child in the xml, but I can't seem to get it to point to the tag and pull it. Any help would be greatly appreciated. I've been working on this for 3 days, granted off someone elses work, but this is the most knowledge I have ever gained from scripting. Thanks fellas!"
13,CVE,learnpython,Script eating up all my ram?,7,6iv8se,https://www.reddit.com/r/learnpython/comments/6iv8se/script_eating_up_all_my_ram/,2017-06-23 03:59:17,"Hey All, I currently have a script that gets sent JSON data, and from that pulls the year (IE, 2011) from the JSON request, and downloads the NIST CVE data and tries to obtain the CPE data. Problem I am running into, is the script for some of hte larger JSON files, is eating up the ram my micro server has. It only has 1.5 gigs of ram. I am wondering if there is a way to prevent it from eating up all my ram? The mass amount of prints is because this is for use with Lambda so its easier to troubleshoot where things went wrong. #!/usr/bin/env python import sys import json import urllib import gzip import os,glob import boto3 import pymysql from datetime import date import boto3 from botocore.client import Config cveDB = pymysql.connect (host=""cve_database"",port=3306,user=cve_login,passwd=cve_password,db='cve',charset='utf8',autocommit=True) cve_cursor = cveDB.cursor() def cpe_parser(cve_data): cpe_list = [] for data_list in cve_data[""CVE_Items""]: for cve_tag,cve_id in data_list[""CVE_data_meta""].items(): cve = str(cve_id) for cve_configurations in data_list[""CVE_configurations""]['CVE_configuration_data']: #First Attempt at CPE data. try: for cve_cpe_data in cve_configurations['cpe']: cve_cpe = cve_cpe_data['cpeMatchString'] cpe_list = cpe_list + [[cve,cve_cpe]] #For some reason CPE data can be served two different ways, so this attempts the second way. except KeyError: try: for cve_cpe_child_data in cve_configurations['children']: for cve_cpe_data in cve_cpe_child_data['cpe']: cve_cpe = str(cve_cpe_data['cpeMatchString']) cpe_list = cpe_list + [[cve,cve_cpe]] #A very small handful have none, so we will just pass them. except KeyError: print('CVE %s failed to pass CPE data.') % cve print('Inserting Data for %s') % cve cve_cursor.executemany('INSERT into cve_cpe (cve_id,cpe_id) VALUES (%s, %s)',cpe_list) print('%s Inerted to DB') % cve #Invokes a Lambda function that does teh MySQL insert. #Downloads the JSON files from the NIST Data Feed Website, and loads it into gzip. def download_json(file_year): print('Downloading ' +str(file_year)) feed_url = 'URL print(feed_url) download_file = ""/tmp/nvdcve-1.0-{}.json.gz"".format(file_year) urllib.urlretrieve(feed_url, download_file) print('Downloaded') latest_file = gzip.open(download_file, 'rb') print('Unpacked') cve_data = json.loads(latest_file.read()) print('JSON Unloaded') #Passes the file to the parser funciton. print('Parsing Data') cpe_parser(cve_data) #main function to get things started. def lambda_handler(json_list,json_details): print(json_list) file_year = json_list['year'] download_json(file_year) cve_cursor.close() cveDB.close()"
14,CVE,learnpython,How to speed up making hundreds of mysql queries?,1,6fwzno,https://www.reddit.com/r/learnpython/comments/6fwzno/how_to_speed_up_making_hundreds_of_mysql_queries/,2017-06-08 08:46:33,"Hey All, I currently have a a MySQL database that is storing data from the NIST database CVSS feed. Right now, to make sure I have the most up-to-date information, I download the latest 'update' file and do a mysql REPLACE command to make sure all the data is accurate and up-to-date, especially since there is so much data. The problem I run into, is this can take 10-15 minutes given how many updates there can be. The problem comes with the time it takes for mysql to search, delete, and add the content. So I am wondering, how can I speed this up? I read a little bit about multi processing, and was wondering if that is the right approach to go with, does it even do what I want, or is there a better way to do it? And example of my code mysql part of the code. cve_cursor.execute('''REPLACE into cve_details (cve_id,cve_package,cve_description,cve_url,cve_published) VALUES (%s, %s, %s, %s, %s)''',(cve_id,application_version,cve_description,vuln_url,publish_date)) Basically, it parses the JSON file, and create a REPLACE with the variable I need for MySQL. A for loop basically says, for every CVE in JSON file, grabt hese details and perform this mysql query. "
15,CVE,learnpython,Another Beginner Shodan Question,0,63dos4,https://www.reddit.com/r/learnpython/comments/63dos4/another_beginner_shodan_question/,2017-04-04 22:06:09,"Hey all, So I posted a quick script here yesterday I wrote to try to use Shodan, and I got some quick responses that really helped. I have dug a little deeper and am trying to make my output a bit cleaner. My goal is to iterate through a list of IP's (in IPlist.txt), and have it run the listed queries and print the desired output. My code runs, but it prints nothing. I get no output anywhere. Here is the code: import shodan import requests SHODAN_API_KEY = ""XXX"" api = shodan.Shodan(SHODAN_API_KEY) with open('IPlist.txt') as target: for line in target.read().splitlines(): try: content = requests.get('URL + line + '?key=XXX').read() hostIP = content.json()[line] host = api.host(hostIP) print ""IP: %s"" % host['ip_str'] print ""Organization: %s"" % host.get('org', 'n/a') print ""Operating System: %s"" % host.get('os', 'n/a') for item in host['data']: print ""Port: %s"" % item['port'] print ""Banner: %s"" % item['data'] for item in host['vulns']: CVE = item.replace('!','') print 'Vulns: %s' % item exploits = api.exploits.search(CVE) for item in exploits['matches']: if item.get('cve')[0] == CVE: print item.get('description') except: 'An error occured' pass Is my indentation wrong again? Again, my code will run with no errors, I just get 0 output. Thanks in advance! I'm trying to level up here :) Also: some of this code I found while doing research so I tried to not reinvent the wheel and use some of it that seemed to make sense to me. Edit: Formatting."
16,CVE,learnpython,Python breaking on CSV file?,1,5kyi4o,https://www.reddit.com/r/learnpython/comments/5kyi4o/python_breaking_on_csv_file/,2016-12-30 05:57:49,"Hey all, I am running into this weird issue. EDIT: I Should also mentioned this worked in the past, so I am also thinking maybe something wrong with the CSV itself? A break down. I have a script that pulls data from a .csv file of CVE (vulnerability) data. It then uses the cvss module to rescore the findings where we use the output as a way to measure priority of patching and urgency. (this script is a temporary fix until we implement new tooling) Here is where it messes up. Here is what my ingest file output looks like right now. Vulnerability Title,Plugin ID,Original CVSS Score,Default Vector,Original Severity,New Score,New Vector,New Severity,Hosts,Host Type,Percentage Impacted Cisco IOS IKEv1 Packet Handling Remote Information Disclosure (cisco-sa-20160916-ikev1) (BENIGNCERTAIN),NES-93736,5.0,CVSS2#AV:N/AC:L/Au:N/C:P/I:N/A:N,,,AV:N/AC:L/Au:N/C:P/I:N/A:N,, Cisco IOS Software TCP Memory Leak DoS (cisco-sa-20150325-tcpleak),NES-82568,7.8,CVSS2#AV:N/AC:L/Au:N/C:N/I:N/A:C,,,AV:N/AC:L/Au:N/C:N/I:N/A:C,,30,26, RHEL 5 / 6 / 7 : nss and nss-util (RHSA-2016:2779),NES-94912,9.3,CVSS2#AV:N/AC:M/Au:N/C:C/I:C/A:C/E:F/RL:OF/RC:ND,,,AV:N/AC:M/Au:N/C:C/I:C/A:C/E:F/RL:OF/RC:ND,,5112,23, And for some reason, various parts of the output are mixed up. If you look at line 2, it starts with which I think comes from the end of line 3, because the end of 3 should say medium, but instead says Medi1, so the ium went to line 2 when it should be at the end of 3 to finish the word. So line 2 starts with a part of line 3 even though line 3 is being written later? Vulnerability Title,Plugin ID,Original CVSS Score,Default Vector,Original Severity,New Score,New Vector,New Severity,Hosts,Host Type,Percentage Impacted ium,4.6,AV:A/AC:H/Au:M/C:P/I:N/A:P/CDP:L/TD:H/CR:H/IR:H/AR:H,Medium,26,26,0.2524271844660194 Cisco IOS Software TCP Memory Leak DoS (cisco-sa-20150325-tcpleak),NES-82568,7.8,CVSS2#AV:N/AC:L/Au:N/C:N/I:N/A:C,High,4.9,AV:A/AC:H/Au:M/C:N/I:N/A:C/CDP:L/TD:M/CR:H/IR:H/AR:H,Medi1 RHEL 5 / 6 / 7 : nss and nss-util (RHSA-2016:2779),NES-94912,9.3,CVSS2#AV:N/AC:M/Au:N/C:C/I:C/A:C/E:F/RL:OF/RC:ND,High,4.2,AV:A/AC:H/Au:M/C:C/I:C/A:C/E:F/RL:OF/RC:ND/CDP:L/TD:M/CR:7 Here is the function in my script that does all of it. Not sure what is breaking. The only thing I can think of is maybe I need to slow a part of the script down somewhere. def rescore_function(): #headers print 'Starting Rescore' csv_in = open('/tmp/rescore_test.csv', 'r') csv_out = open('/tmp/rescored_vulnerabilities.csv', 'w') writer = csv.writer(csv_out) reader = csv.reader(csv_in) headers = next(reader, None) if headers: writer.writerow(headers) print 'Creating Target Distrobution' for row in csv.reader(csv_in): #This is a terrible way of setting up the percentage of hosts impacted for target distrobution. Its ugly and horrible. Host count defines the host impacted, host_type identifies what kind of host it is. Such as Alinux, Rhel5, or Cisco IOS host_count = float(row[8]) host_type = float(row[9]) alinux_impact = host_count / ALINUX_HOST cisco_impact = host_count / CISCO_COUNT juniper_impact = host_count / JUNIPER_COUNT citrix_impact = host_count / CITRIX_COUNT all_linux= host_count / LINUX_TOTAL print 'math set' #The reason for vul_id is 3 lists combined is simple. alinux_impact NEEDS to be 24, cisco NEEDs to be 26, juniper NEEDS to match 27, because vul_id is the softwares 'vulnerability ID type #range falls into all_linux. So fillvalue=vul_os[-1] means if its not 24,26,27, it is ""all_linux"" which means it compares it to the All linux number. vul_id = [24, 26, 27, 25] + range(24) + range(28,101) vul_os = [alinux_impact, cisco_impact, juniper_impact, all_linux] append_file = open('/tmp/rescored_vulnerabilities.csv', 'a') append_write = csv.writer(append_file) #Does the for loop with the fillvalue as mentioned above. Basically Y is the host type (Alinux, Cisco IOS, etc) and X is the vulnerability type. So it runs through and figures out the TD and rescore methods. #X equals the percetange of impacted, so the Metric will be based on amount/percentage of X impacted and does a regex search and replace based on that using the CVSS calculations. print vul_id print vul_os for x,y in izip_longest(vul_os, vul_id, fillvalue=vul_os[-1]): print x,y print host_type #VECTOR REGEXP, host_type is which OS/Device type. 23 = RHEL5, 24 = Alinux, 26 = Cisco, 27 = Juniper if host_type == y: row[10] = x if x &lt;= 0.25: AC_Metric = 'A:C/CDP:L/TD:L/CR:H/IR:H/AR:H' AP_Metric = 'A:P/CDP:L/TD:L/CR:H/IR:H/AR:H' AN_Metric = 'A:N/CDP:L/TD:L/CR:H/IR:H/AR:H' RCUC_Metric = 'RC:UC/CDP:L/TD:L/CR:H/IR:H/AR:H' RCUR_Metric = 'RC:UR/CDP:L/TD:L/CR:H/IR:H/AR:H' RCC_Metric = 'RC:C/CDP:L/TD:L/CR:H/IR:H/AR:H' RCND_Metric = 'RC:ND/CDP:L/TD:L/CR:H/IR:H/AR:H' elif 0.26 &lt;= x &lt;= 0.75: AC_Metric = 'A:C/CDP:L/TD:M/CR:H/IR:H/AR:H' AP_Metric = 'A:P/CDP:L/TD:M/CR:H/IR:H/AR:H' AN_Metric = 'A:N/CDP:L/TD:M/CR:H/IR:H/AR:H' RCUC_Metric = 'RC:UC/CDP:L/TD:M/CR:H/IR:H/AR:H' RCUR_Metric = 'RC:UR/CDP:L/TD:M/CR:H/IR:H/AR:H' RCC_Metric = 'RC:C/CDP:L/TD:M/CR:H/IR:H/AR:H' RCND_Metric = 'RC:ND/CDP:L/TD:M/CR:H/IR:H/AR:H' else: AC_Metric = 'A:C/CDP:L/TD:H/CR:H/IR:H/AR:H' AP_Metric = 'A:P/CDP:L/TD:H/CR:H/IR:H/AR:H' AN_Metric = 'A:N/CDP:L/TD:H/CR:H/IR:H/AR:H' RCUC_Metric = 'RC:UC/CDP:L/TD:H/CR:H/IR:H/AR:H' RCUR_Metric = 'RC:UR/CDP:L/TD:H/CR:H/IR:H/AR:H' RCC_Metric = 'RC:C/CDP:L/TD:H/CR:H/IR:H/AR:H' RCND_Metric = 'RC:ND/CDP:L/TD:H/CR:H/IR:H/AR:H' text = row[6] text = re.sub(r'AV:N','AV:A',text) text = re.sub(r'AC:L','AC:H',text) text = re.sub(r'AC:M','AC:H',text) text = re.sub(r'Au:N','Au:M',text) text = re.sub(r'Au:S','Au:M',text) text = re.sub(r'A:C$',AC_Metric,text) text = re.sub(r'A:P$',AP_Metric,text) text = re.sub(r'A:N$',AP_Metric,text) text = re.sub(r'RC:UC',RCUC_Metric,text) text = re.sub(r'RC:UR',RCUR_Metric,text) text = re.sub(r'RC:C',RCC_Metric,text) text = re.sub(r'RC:ND',RCND_Metric,text) row[6] = text #NEW SCORE, uses CVSS module to take the previous vector and find out the the numbered score. It then uses that number to define the severity word. try: vector = row[6] c = CVSS2(vector) row[5] = c.scores()[2] vul_score = row[5] if 0 &lt;= vul_score &lt;= 3.9: vuln_word = 'Low' elif 4.0 &lt;= vul_score &lt;=6.9: vuln_word = 'Medium' elif 7.0 &lt;= vul_score &lt;= 9.9: vuln_word = 'High' else: vuln_word = 'Critical' row[7] = vuln_word except CVSS2MalformedError: rescored_success = False pass #ORIGINAL SCORE, does the same as above for the original vector since NESSUS does not provide the Severity ""word"". This only finds the word, not the number value. default_score = float(row[2]) if 0 &lt;= default_score &lt;= 3.9: default_severity = 'Low' elif 4.0 &lt;= default_score &lt;=6.9: default_severity = 'Medium' elif 7.0 &lt;= default_score &lt;= 9.9: default_severity = 'High' else: default_severity = 'Critical' row[4] = default_severity append_write.writerow(row) "
